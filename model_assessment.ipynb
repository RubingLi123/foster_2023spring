{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "name": "Model_assessment.ipynb"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jefD2j6NLvMd"
      },
      "source": [
        "To open notebook in Colab please click below:  \n",
        "<a href=\"https://colab.research.google.com/github/pearl-yu/foster_2022fall/blob/2022-master/Module5_ROC_Cost_Visualization/Model_assessment.ipynb\" target=\"_parent\"> <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\" /> </a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "InCqufkHLvMm"
      },
      "source": [
        "#If opening in colab run this cell\n",
        "!git clone https://github.com/pearl-yu/foster_2022fall\n",
        "%cd foster_2022fall/Module5_ROC_Cost_Visualization/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DFaN0JqULvNI"
      },
      "source": [
        "# Model Assessment\n",
        "\n",
        "\n",
        "Fall 2022 - Instructors: Foster Provost and Pearl Yu\n",
        "Teaching Assistant: Pearl Yu\n",
        "\n",
        "***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7RO4OlvMLvNN"
      },
      "source": [
        "Import all of the packages we will need."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PtodL8PHLvNR",
        "scrolled": false
      },
      "source": [
        "# Import the libraries we will be using\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.svm import SVC\n",
        "from sklearn import metrics\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Our custom libraries!\n",
        "import sys\n",
        "sys.path.append(\"..\")\n",
        "from ds_utils.sample_data import *\n",
        "\n",
        "import matplotlib.pylab as plt\n",
        "%matplotlib inline\n",
        "plt.rcParams['figure.figsize'] = 15, 12"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Orw_IlwULvNZ"
      },
      "source": [
        "### Data\n",
        "We're going to use a mail response data set from a real direct marketing campaign located in `data/mailing.csv`. Each record represents an individual who was targeted with a direct marketing offer.  The offer was a solicitation to make a charitable donation. \n",
        "\n",
        "The columns (features) are:\n",
        "\n",
        "```\n",
        "income       household income\n",
        "Firstdate    data assoc. with the first gift by this individual\n",
        "Lastdate     data associated with the most recent gift \n",
        "Amount       average amount by this individual over all periods (incl. zeros)\n",
        "rfaf2        frequency code\n",
        "rfaa2        donation amount code\n",
        "pepstrfl     flag indicating a star donator\n",
        "glast        amount of last gift\n",
        "gavr         amount of average gift\n",
        "```\n",
        "\n",
        "The target variables is `class` and is equal to one if they gave in this campaign and zero otherwise."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tTxcwq11LvNd",
        "scrolled": false
      },
      "source": [
        "# Load the data\n",
        "data = pd.read_csv(\"data/mailing.csv\")\n",
        "# Let's take a look at the data\n",
        "data.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dDzQ2Ax2LvNm"
      },
      "source": [
        "From the description above, and the head of the data, we see that two of the fields are **categorical** instead of typical **numerical** fields. Today, one of the models we will be using is a logistic regression. From the previous classes, we have seen that logistic regression requires *all* fields to be numerical. So, we are going to create \"dummy\" variables for all the fields that are categorical (same as you did for your homework).  [I believe in sklearn all models require numeric variables; that's not the case for every implementation.]\n",
        "\n",
        "#### Dummyize\n",
        "A dummy variable is a binary variable corresponding to one value of a categorical variable.\n",
        "The typical way to create dummies for a field is to create new variables for each possible category of the field. For example consider a field called color that can have the possible values \"red\", \"blue\", and \"green\". To dummyize color, we would create three new features: \"color_red\", \"color_blue\", and \"color_green\". These fields would take the value 1 or 0 depending on the actual value of color. Each record can only have one of these fields set to 1.\n",
        "\n",
        "Notes:\n",
        "\n",
        "- You can also leave out one of the possible categories. For example, in the above example that had three possible values, you can create only two dummies. This, because when \"color_red\"=0 and \"color_blue\"=0 it means that \"color_green=1\".  Often all three dummies are created anyway; it is slightly redundant, but makes the models more comprehensible.\n",
        "\n",
        "- There also are cases where non-numeric variables can take on multiple values (for example, `colors = {red, white, blue}`).  In these cases again often binary variables are created for each value, the obvious difference being that now more than one can be non-zero (and you would need to represent all the values).\n",
        " \n",
        "\n",
        "So.  Let's dummyize the fields `rfaa2` and `pepstrfl`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0JDBX7gFLvNp",
        "scrolled": false
      },
      "source": [
        "for field in ['rfaa2', 'pepstrfl']:\n",
        "    dummies = pd.get_dummies(data[field])\n",
        "    dummies.columns = [field + \"_\" + s for s in dummies.columns]\n",
        "    data = pd.concat([data, dummies], axis=1).drop(field, axis=\"columns\")\n",
        "    \n",
        "data.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u_Yw7qSwLvNy"
      },
      "source": [
        "### Confusion matrices\n",
        "Let's build a confusion matrix using a logistic regression model. \n",
        "\n",
        "**Important and overlooked (always remember this!):** a confusion matrix is defined with respect to a classifier, not a scoring model.  However, our models *are* scoring models (e.g., class-probability estimation models).  So the confusion matrices are defined with respect a scoring model plus a *threshold* on the score.  The threshold should be chosen carefully, and with the business need in mind.   For binary classes, the default for most modeling programs when they return a predicted classification is to use a threshold corresponding to an estimated class probability of 0.5.  This is because the modeling program does not know the business setting, and 0.5 makes sense as a default (in expectation it gives the maximum classification accuracy, if the probabilities are well calibrated).  However, 0.5 probably is not the right threshold for any particular problem.\n",
        "\n",
        "So let's start with the default of predicting a 1 if the estimated probability is $\\geq$ 50% and a 0 otherwise.\n",
        "\n",
        "Remember, a confusion matrix looks like:\n",
        "\n",
        "```\n",
        "  |____________ p __________|___________ n ___________|\n",
        "Y |   1's above threshold   |   0's above threshold   |\n",
        "N |   1's below threshold   |   0's below threshold   |\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t9Z6nZYGLvN2",
        "scrolled": false
      },
      "source": [
        "# Split our data into training and test sets\n",
        "X = data.drop(['class'], axis=1)\n",
        "Y = data['class']\n",
        "X_mailing_train, X_mailing_test, Y_mailing_train, Y_mailing_test = train_test_split(X, Y, test_size=.25, random_state=42)\n",
        "\n",
        "# Make and fit a model on the training data\n",
        "model_mailing = LogisticRegression(C=1000000, solver='liblinear')\n",
        "model_mailing.fit(X_mailing_train, Y_mailing_train)\n",
        "\n",
        "# Get probabilities of being a donor (We saw this in a prior last class)\n",
        "probabilities = model_mailing.predict_proba(X_mailing_test)[:, 1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UjrfsirxLvOA"
      },
      "source": [
        "Use the default threshold of 50% to decide Y vs N.\n",
        "\n",
        "(An individual below this threshold will get a prediction of \"0\" and someone above this will get a prediction of \"1\")"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_LampQbELvOD",
        "scrolled": false
      },
      "source": [
        "prediction = probabilities > 0.5\n",
        "\n",
        "# Build and print a confusion matrix\n",
        "confusion_matrix_50 = pd.DataFrame(metrics.confusion_matrix(Y_mailing_test, prediction, labels=[1, 0]).T,\n",
        "                                columns=['p', 'n'], index=['Y', 'N'])\n",
        "print (confusion_matrix_50)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ztIEkLytLvOL"
      },
      "source": [
        "Wait -- take a close look at that.  What's going on?\n",
        "\n",
        "Incidentally, what would be the classification accuracy here?\n",
        "\n",
        "-\n",
        "\n",
        "-\n",
        "\n",
        "-\n",
        "\n",
        "-\n",
        "\n",
        "-\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bpXz9IJjLvON"
      },
      "source": [
        "What if we lower the threshold to 5%?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iDIM1_M-LvOQ",
        "scrolled": false
      },
      "source": [
        "# Let's move the threshold down\n",
        "prediction = probabilities > 0.05\n",
        "\n",
        "# Build and print a confusion matrix\n",
        "confusion_matrix_5 = pd.DataFrame(metrics.confusion_matrix(Y_mailing_test, prediction, labels=[1, 0]).T,\n",
        "                                columns=['p', 'n'], index=['Y', 'N'])\n",
        "print (confusion_matrix_5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XT8Fl3iHLvOZ"
      },
      "source": [
        "***\n",
        "Is this good performance? \n",
        "\n",
        "How can we tell?\n",
        "\n",
        "(Incidentally, what would be the classification accuracy now?)\n",
        "\n",
        "Is 5% the right threshold?  How would we determine that?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ymJpg0gGLvOc"
      },
      "source": [
        "--------------------------------------------------------------------------\n",
        "### Other measures: Receiver operating characteristic (ROC) curves\n",
        "\n",
        "In the book, we were trying to predict if customers should be given a credit card.  Let's use a version of that example here:\n",
        "\n",
        "- Target: `Y = 1` \n",
        "- Three features: \"earning\", \"geographic\", and \"experience\".\n",
        "\n",
        "Up until this point, when we need a \"single number metric\" for generalization performance, we have been using \"vanilla\" classification accuracy (the number of records correctly classified divided by the total number of records). However, classification accuracy usually does not  give us the \"best\" interpretation of our model's performance for a particular business problem. An more general evaluation is to visualize and measure the performance of a model using the Reciever Operating Characteristic **(ROC) curve**. \n",
        "\n",
        "Let's first specify how we create ROC curves: For each threshold $t$ that is chosen, we can define two quantities. First, the false positive rate, $FPR = \\frac{False Positives}{False Positives+True Negatives}$, and second, the true positive rate, $TPR = \\frac{True Positives}{True Positives+False Negatives}$. The ROC curve is the result of plotting $FPR$ against $TPR$ for each value of $t$ that is possible in the data.  It helps us to visualize and analyze the trade-offs between the opportunity for benefits (via true positives) and the possibility of costs (via false positives). \n",
        "\n",
        "\" The lower left point **(0, 0) represents the strategy of never issuing a positive classification**; such a classifier commits no false positive errors but also gains no true positives. The opposite strategy, of unconditionally issuing positive classifications, is represented by the upper right point (1, 1). The point **(0, 1) represents perfect classification** (the star in Figure 8-3). The diagonal line connecting (0, 0) to (1, 1) represents the policy of guessing a class (for example, by flipping a weighted coin). \"  \n",
        "\n",
        "- _Provost, Foster, and Tom Fawcett. Data Science for Business: _\n",
        "  _What you need to know about data mining and data-analytic thinking. O'Reilly Media, Inc., 2013._\n",
        "\n",
        "\n",
        "\n",
        "<img src=\"https://github.com/pearl-yu/foster_2022fall/blob/2022-master/Module5_ROC_Cost_Visualization/images/ROC1.png?raw=1\" alt=\"Drawing\" style=\"width: 600px;\"/>\n",
        "\n",
        "<img src=\"https://github.com/pearl-yu/foster_2022fall/blob/2022-master/Module5_ROC_Cost_Visualization/images/ROC2.png?raw=1\" alt=\"Drawing\" style=\"width: 600px;\"/>\n",
        "<img src=\"https://github.com/pearl-yu/foster_2022fall/blob/2022-master/Module5_ROC_Cost_Visualization/images/ROC3.png?raw=1\" alt=\"Drawing\" style=\"width: 600px;\"/>\n",
        "\n",
        "\n",
        "Doing this in **sklearn** is relatively straightforward.\n",
        "\n",
        "Let's create a **new DATA SET** for this, that will show differences between different models.  We will build logistic regression models with different regularization parameters."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q7JJQ429LvOf",
        "scrolled": false
      },
      "source": [
        "X, Y = get_dummy_data()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hoARtRRy1ib0"
      },
      "source": [
        "X.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hN0MKedJLvOm"
      },
      "source": [
        "We can now build and fit a model. Using this model, we will plot an *ROC curve*. \n",
        "\n",
        "[Sidebar question: Why are we taking the mean from the `cross_validation.cross_val_score` output below?]"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yDmb75RBLvOp",
        "scrolled": false
      },
      "source": [
        "#Split train and test\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=.2)\n",
        "\n",
        "# Fit a logistic regression model\n",
        "for c in [0.01, 0.05, .1, 1]:\n",
        "    model = LogisticRegression(C=c, solver='liblinear')\n",
        "    model.fit(X_train, Y_train)\n",
        "\n",
        "    # Get the probability of Y_test records being = 1\n",
        "    Y_test_probability_1 = model.predict_proba(X_test)[:, 1]\n",
        "\n",
        "    # Use the metrics.roc_curve function to get the true positive rate (tpr) and false positive rate (fpr)\n",
        "    fpr, tpr, thresholds = metrics.roc_curve(Y_test, Y_test_probability_1)\n",
        "    \n",
        "    # Compute (estimate) the area under the curve (AUC)\n",
        "    auc = np.mean(cross_val_score(model, X, Y, scoring=\"roc_auc\", cv=5))\n",
        "\n",
        "    # Plot the ROC curve\n",
        "    plt.plot(fpr, tpr, label=\"AUC (C=\" + str(c) + \") = \" + str(round(auc, 2)))\n",
        "    \n",
        "plt.xlabel(\"False positive rate (fpr)\")\n",
        "plt.ylabel(\"True positive rate (tpr)\")\n",
        "plt.plot([0,1], [0,1], 'k--', label=\"Random\")\n",
        "plt.legend(loc='best')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dN3wVwP2LvOx"
      },
      "source": [
        "### Other measures: Cumulative response and lift curves\n",
        "The interpretation of an ROC curve often is not intuitive to business stakeholders. In many applications the **cumulative response curve** is more useful.  It is very close to an ROC curve, except the x-axis is the percentage of all instances that are above each threshold.  \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OBn8PB3qLvOz",
        "scrolled": false
      },
      "source": [
        "def build_cumulative_curve(model, scale=100):\n",
        "    # Fit model\n",
        "    model.fit(X_train, Y_train)\n",
        "\n",
        "    # Get the probability of Y_test records being = 1\n",
        "    Y_test_probability_1 = model.predict_proba(X_test)[:, 1]\n",
        "\n",
        "    # Sort theseprobabilities and the true value in descending order of probability\n",
        "    order = np.argsort(Y_test_probability_1)[::-1]\n",
        "    Y_test_probability_1_sorted = Y_test_probability_1[order]\n",
        "    Y_test_sorted = np.array(Y_test)[order]\n",
        "\n",
        "    # Build the cumulative response curve\n",
        "    x_cumulative = np.arange(len(Y_test_probability_1_sorted)) + 1\n",
        "    y_cumulative = np.cumsum(Y_test_sorted)\n",
        "\n",
        "    # Rescale\n",
        "    x_cumulative = np.array(x_cumulative)/float(x_cumulative.max()) * scale\n",
        "    y_cumulative = np.array(y_cumulative)/float(y_cumulative.max()) * scale\n",
        "    \n",
        "    return x_cumulative, y_cumulative\n",
        "\n",
        "def plot_cumulative_curve(models):\n",
        "    # Plot curve for each model\n",
        "    for key in models:\n",
        "        x_cumulative, y_cumulative = build_cumulative_curve(models[key])\n",
        "        plt.plot(x_cumulative, y_cumulative, label=key)\n",
        "    # Plot other details\n",
        "    plt.plot([0,100], [0,100], 'k--', label=\"Random\")\n",
        "    plt.xlabel(\"Percentage of test instances targeted (decreasing score)\")\n",
        "    plt.ylabel(\"Percentage of positives targeted\")\n",
        "    plt.title(\"Cumulative response curve\")\n",
        "    plt.legend()\n",
        "\n",
        "models = {\"Logistic Regression\": LogisticRegression(C=1.0, solver=\"liblinear\")}\n",
        "plot_cumulative_curve(models)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cOJBGFUdLvO8"
      },
      "source": [
        "We can also easily plot a **lift curve** in this scenario."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W0RQHeENLvO_",
        "scrolled": false
      },
      "source": [
        "def plot_lift_curve(models):\n",
        "    # Plot curve for each model\n",
        "    for key in models:\n",
        "        x_cumulative, y_cumulative = build_cumulative_curve(models[key])\n",
        "        plt.plot(x_cumulative, y_cumulative/x_cumulative, label=key)\n",
        "    # Plot other details\n",
        "    plt.plot([0,100], [1,1], 'k--', label=\"Random\")\n",
        "    plt.xlabel(\"Percentage of test instances (decreasing score)\")\n",
        "    plt.ylabel(\"Lift (times)\")\n",
        "    plt.title(\"Lift curve\")\n",
        "    plt.legend()\n",
        "\n",
        "plot_lift_curve(models)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R-2XOy5jLvPG"
      },
      "source": [
        "Now, let's revisit our **mailing dataset** compare the cumulative response curves and then the lift curves of two models (a logistic regression model and a tree)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5IjO8YjKLvPI",
        "scrolled": false
      },
      "source": [
        "\n",
        "X_train, X_test, Y_train, Y_test = X_mailing_train, X_mailing_test, Y_mailing_train, Y_mailing_test\n",
        "\n",
        "models = {\"Logistic Regression\": LogisticRegression(C=1.0, solver=\"liblinear\"), \n",
        "          \"Classification Tree\": DecisionTreeClassifier(criterion=\"entropy\", min_samples_leaf=300, random_state=42)\n",
        "          }\n",
        "plot_cumulative_curve(models)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "spbQzLSqLvPT",
        "scrolled": false
      },
      "source": [
        "plot_lift_curve(models)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iens5XJBLvPb"
      },
      "source": [
        "What if we want to understand not just lift, but how much benefit we are going to receive from a certain investment in targeting?  We can plot a profit curve.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UmBnUSrGLvPc"
      },
      "source": [
        "### Profit curves\n",
        "Let's say that each offer costs \\$1 to make and market, and each accepted offer earns \\$18, for a profit of $17. The cost matrix would be:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d83AHt5ILvPe",
        "scrolled": false
      },
      "source": [
        "unit_cost = 1\n",
        "unit_revenue = 18\n",
        "\n",
        "cost_matrix = pd.DataFrame([[unit_revenue - unit_cost, - unit_cost], [0, 0]], columns=['p', 'n'], index=['Y', 'N'])\n",
        "print (\"Cost matrix\")\n",
        "print (cost_matrix)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aa9L3OiGLvPm"
      },
      "source": [
        "Remember that we examined different targeting thresholds (aka \"decision thresholds\"): "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qXChRdV7LvPo",
        "scrolled": false
      },
      "source": [
        "print (\"Confusion matrix with threshold = 50% to predict labels\")\n",
        "print (confusion_matrix_50)\n",
        "print (\"\\n\")\n",
        "print (\"Confusion matrix with threshold = 5% to predict labels\")\n",
        "print (confusion_matrix_5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G6ab6DP2LvPt"
      },
      "source": [
        "Based on those predictions, the expected profit of using 50% and 5% as your decision threshold would be."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "97_i9_e6LvPv",
        "scrolled": false
      },
      "source": [
        "profit_w_50 = np.sum((confusion_matrix_50 * cost_matrix).values)\n",
        "profit_w_5 = np.sum((confusion_matrix_5 * cost_matrix).values)\n",
        "\n",
        "print (\"Expected profit per targeted individual with a cutoff of 50%% is $%.2f.\" % profit_w_50)\n",
        "print (\"Expected profit per targeted individual with a cutoff of 5%% is $%.2f.\" % profit_w_5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6t3JqmMRLvP1"
      },
      "source": [
        "And here is the profit curve."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tHjoTZUlLvP3"
      },
      "source": [
        "unit_cost = -cost_matrix['n']['Y']\n",
        "unit_revenue = cost_matrix['p']['Y'] + unit_cost\n",
        "\n",
        "def plot_profit_curve(models):\n",
        "    # Plot curve for each model\n",
        "    total_obs = len(Y_test)\n",
        "    total_pos = Y_test.sum()\n",
        "    for key in models:\n",
        "        x_cumulative, y_cumulative = build_cumulative_curve(models[key], scale=1)\n",
        "        profits = unit_revenue * y_cumulative * total_pos - unit_cost * x_cumulative * total_obs\n",
        "        plt.plot(x_cumulative*100, profits, label=key)\n",
        "    # Plot other details\n",
        "    plt.xlabel(\"Percentage of users targeted\")\n",
        "    plt.ylabel(\"Profit\")\n",
        "    plt.title(\"Profit curve\")\n",
        "    plt.legend()\n",
        "    \n",
        "plot_profit_curve(models)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hwLaH1vWLvP9"
      },
      "source": [
        "Which one do you think we should choose? Why?"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "unit_cost = -cost_matrix['n']['Y']\n",
        "unit_revenue = cost_matrix['p']['Y'] + unit_cost\n",
        "\n",
        "def plot_profit_curve(models):\n",
        "    # Plot curve for each model\n",
        "    total_obs = len(Y_test)\n",
        "    total_pos = Y_test.sum()\n",
        "    for key in models:\n",
        "        x_cumulative, y_cumulative = build_cumulative_curve(models[key], scale=1)\n",
        "        \n",
        "        profits = unit_revenue * y_cumulative * total_pos - unit_cost * x_cumulative * total_obs\n",
        "\n",
        "        plt.plot(x_cumulative*100, profits, label=key)\n",
        "\n",
        "        # Find the estimated probability\n",
        "        Y_test_probability_1 = models[key].predict_proba(X_test)[:, 1]\n",
        "        order = np.argsort(Y_test_probability_1)[::-1]\n",
        "        Y_test_probability_1_sorted = Y_test_probability_1[order]\n",
        "        # add the point to the plot\n",
        "        plt.plot(x_cumulative[profits.argmax(axis=0)]*100,\n",
        "                 profits[profits.argmax(axis=0)],\n",
        "                 'ro')\n",
        "        plt.text(x_cumulative[profits.argmax(axis=0)]*100,\n",
        "                 profits[profits.argmax(axis=0)],\"Estimated probability is %.4f.\" % Y_test_probability_1_sorted[profits.argmax(axis=0)],horizontalalignment='right')\n",
        "\n",
        "    # Plot other details\n",
        "    plt.xlabel(\"Percentage of users targeted\")\n",
        "    plt.ylabel(\"Profit\")\n",
        "    plt.title(\"Profit curve\")\n",
        "    plt.legend()\n",
        "    \n",
        "plot_profit_curve(models)"
      ],
      "metadata": {
        "id": "y1YC2r86HQY7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YRX7PYNvO_ew"
      },
      "source": [
        "#save for later\n",
        "mailing_y_train, mailing_y_test, mailing_X_train, mailing_X_test = Y_train, Y_test, X_train, X_test"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yO5I9mIpLvP_"
      },
      "source": [
        "## Calibration Curves"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ya1a4rxiLvQB"
      },
      "source": [
        "But going back to probabilities for a second. Another measure of how well a model fits the data is how accurate its predicted probabilities are. If for an instance with feature vector X, a logistic regression predicts a positive outcome with 0.40 probability, in an ideal world 40% of the instances with feature vector X would be positive.  We might not have a lot of instances with the same feature vector, but also of the instances assigned a 0.4 estimated probability, 40% should be positive.  Let's go back to the titanic dataset and take a look.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r5-ytv8sLvQC"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn import preprocessing\n",
        "from sklearn import model_selection\n",
        "from sklearn.svm import LinearSVC\n",
        "\n",
        "path = \"./data/titanic.csv\"\n",
        "df = pd.read_csv(path)[[\"survived\", \"pclass\", \"sex\", \"age\", \"fare\"]].dropna()\n",
        "# Transform sex column to a numeric variable\n",
        "df[\"female\"] = (df.sex == \"female\").astype(int)\n",
        "df = df.drop(\"sex\", axis=\"columns\")\n",
        "# Drop outliers. This is to help the visualization in the next examples.\n",
        "df = df[df.fare < 400]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "863Hn5SeLvQI"
      },
      "source": [
        "from sklearn.calibration import calibration_curve\n",
        "from sklearn.metrics import (brier_score_loss,roc_auc_score)\n",
        "\n",
        "predictor_cols = [\"pclass\", \"age\", \"fare\"]\n",
        "y = df[\"survived\"]\n",
        "y_train,y_test,X_train,X_test = model_selection.train_test_split(y,df[predictor_cols])\n",
        "\n",
        "def plot_calibration_curve(est, name, fig_index, num_bins=10):\n",
        "    \"\"\"Plot calibration curve. \"\"\"\n",
        "\n",
        "    fig = plt.figure(fig_index, figsize=(10, 10))\n",
        "    ax1 = plt.subplot2grid((3, 1), (0, 0), rowspan=2)\n",
        "    ax2 = plt.subplot2grid((3, 1), (2, 0))\n",
        "\n",
        "    ax1.plot([0, 1], [0, 1], \"k:\", label=\"Perfectly calibrated\")\n",
        "    for clf, name in [(est, name)]:\n",
        "        clf.fit(X_train, y_train)\n",
        "        y_pred = clf.predict(X_test)\n",
        "        if hasattr(clf, \"predict_proba\"):\n",
        "            prob_pos = clf.predict_proba(X_test)[:, 1]\n",
        "        else:  # use decision function\n",
        "            prob_pos = clf.decision_function(X_test)\n",
        "            prob_pos = \\\n",
        "                (prob_pos - prob_pos.min()) / (prob_pos.max() - prob_pos.min())\n",
        "\n",
        "        clf_score = brier_score_loss(y_test, prob_pos, pos_label=y.max())\n",
        "        auc_score = roc_auc_score(y_test,prob_pos)\n",
        "        print(\"%s:\" % name)\n",
        "        print(\"\\tBrier score: %1.3f\" % (clf_score))\n",
        "        print(\"\\tROC AUC score: %1.3f\" % (auc_score))\n",
        "\n",
        "        fraction_of_positives, mean_predicted_value = \\\n",
        "            calibration_curve(y_test, prob_pos, n_bins=num_bins)\n",
        "\n",
        "        ax1.plot(mean_predicted_value, fraction_of_positives, \"s-\",\n",
        "                 label=\"%s (%1.3f)\" % (name, clf_score))\n",
        "\n",
        "        ax2.hist(prob_pos, range=(0, 1), bins=num_bins, label=name,\n",
        "                 histtype=\"step\", lw=2)\n",
        "\n",
        "    ax1.set_ylabel(\"Fraction of positives\")\n",
        "    ax1.set_ylim([-0.05, 1.05])\n",
        "    #ax1.set_xlim([-0.05, .35])\n",
        "    ax1.legend(loc=\"lower right\")\n",
        "    ax1.set_title('Calibration plots  (reliability curve)')\n",
        "\n",
        "    ax2.set_xlabel(\"Mean predicted value\")\n",
        "    ax2.set_ylabel(\"Count\")\n",
        "    #ax2.set_xlim([-0.05, .35])\n",
        "    ax2.legend(loc=\"upper center\", ncol=2)\n",
        "\n",
        "    plt.tight_layout()\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot calibration curve \n",
        "plot_calibration_curve( LogisticRegression(C=1.0, solver=\"liblinear\",max_iter=10000), \"Logistic Regression\", 1)"
      ],
      "metadata": {
        "id": "Op2DuTksNCb-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-DkScp0ILvQN"
      },
      "source": [
        "#Now, how about calibration curves on our mailing data?\n",
        "y_train, y_test, X_train, X_test = mailing_y_train, mailing_y_test, mailing_X_train, mailing_X_test \n",
        "# Plot calibration curve \n",
        "plot_calibration_curve( LogisticRegression(C=1.0, solver=\"liblinear\",max_iter=10000), \"Logistic Regression\", 1, num_bins=10)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}